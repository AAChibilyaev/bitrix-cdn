#!/usr/bin/env python3
"""
–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π AI-–∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å –Ω–æ–≤—ã–º OpenAI API
"""

import logging
from typing import Dict, Any, List, Optional
from openai import AsyncOpenAI
from tenacity import retry, stop_after_attempt, wait_exponential

from prometheus_client import PrometheusClient
from docker_client import DockerClient
from models import (
    AppConfig, ContainerInfo, HealthCheckResult, AllMetrics, 
    CacheStatistics, AISystemAnalysis, AITrendAnalysis, AIRecommendation
)
from ai_cache import cached_system_analysis, cached_trend_analysis

logger = logging.getLogger(__name__)

class AIAnalyzer:
    """–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π AI-–∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å –Ω–æ–≤—ã–º OpenAI API"""
    
    def __init__(self, config: AppConfig):
        self.config = config
        self.prometheus = PrometheusClient(config)
        self.docker = DockerClient(config)
        
        # –ù–æ–≤—ã–π AsyncOpenAI client (1 —Å—Ç—Ä–æ–∫–∞)
        self.client = AsyncOpenAI(api_key=config.openai.api_key)
        
        logger.info("Modern AI Analyzer initialized with new OpenAI API")
    
    async def analyze_system(self, use_cache: bool = True) -> AISystemAnalysis:
        """–ê–Ω–∞–ª–∏–∑ —Å–∏—Å—Ç–µ–º—ã - —Ç–µ–ø–µ—Ä—å 3 —Å—Ç—Ä–æ–∫–∏ –≤–º–µ—Å—Ç–æ 70"""
        context = await self._prepare_context_data()
        
        if use_cache:
            result = await cached_system_analysis(context)
            if result:
                return result
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π OpenAI API —Å Structured Outputs
        result = await self._call_openai_with_structured_output(context)
        return result
    
    async def analyze_trends(self, time_range: str = "1h") -> AITrendAnalysis:
        """–ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ —Å–∏—Å—Ç–µ–º—ã"""
        context = f"–ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ –∑–∞ –ø–µ—Ä–∏–æ–¥: {time_range}\n"
        context += await self._prepare_context_data()
        
        result = await self._call_openai_trends(context)
        return result
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    async def _call_openai_with_structured_output(self, context: str) -> AISystemAnalysis:
        """–í—ã–∑–æ–≤ OpenAI —Å Structured Outputs"""
        try:
            response = await self.client.chat.completions.create(
                model=self.config.openai.model,
                messages=[
                    {"role": "system", "content": """–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ DevOps –∏ —Å–∏—Å—Ç–µ–º–∞–º CDN. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã Bitrix CDN –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å:

1. –û–ë–©–ò–ô –°–¢–ê–¢–£–° —Å–∏—Å—Ç–µ–º—ã (healthy/warning/critical)
2. –û–¶–ï–ù–ö–£ –∑–¥–æ—Ä–æ–≤—å—è –æ—Ç 0 –¥–æ 100
3. –ü–†–û–ë–õ–ï–ú–´ (–µ—Å–ª–∏ –µ—Å—Ç—å) —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏
4. –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ø–æ —É–ª—É—á—à–µ–Ω–∏—é
5. –ü–†–û–ì–ù–û–ó –≤–æ–∑–º–æ–∂–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º

–û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –±—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∏ –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–º. –í–ê–ñ–ù–û: –û—Ç–≤–µ—á–∞–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{
  "status": "healthy|warning|critical",
  "overall_health_score": 85,
  "problems": ["–ø—Ä–æ–±–ª–µ–º–∞ 1", "–ø—Ä–æ–±–ª–µ–º–∞ 2"],
  "recommendations": ["—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è 1", "—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è 2"],
  "forecast": "–ø—Ä–æ–≥–Ω–æ–∑ —Å–∏—Ç—É–∞—Ü–∏–∏"
}"""},
                    {"role": "user", "content": context}
                ],
                max_tokens=2000,
                temperature=0.7
            )
            
            # –ü–∞—Ä—Å–∏–º JSON –æ—Ç–≤–µ—Ç
            import json
            content = response.choices[0].message.content
            data = json.loads(content)
            
            return AISystemAnalysis(
                status=data.get("status", "unknown"),
                overall_health_score=data.get("overall_health_score", 0),
                problems=data.get("problems", []),
                recommendations=data.get("recommendations", []),
                forecast=data.get("forecast")
            )
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ OpenAI –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return AISystemAnalysis(
                status="error",
                overall_health_score=0,
                problems=[f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}"],
                recommendations=[],
                forecast=None
            )
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    async def _call_openai_trends(self, context: str) -> AITrendAnalysis:
        """–í—ã–∑–æ–≤ OpenAI –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤"""
        try:
            response = await self.client.chat.completions.create(
                model=self.config.openai.model,
                messages=[
                    {"role": "system", "content": """–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —Ç—Ä–µ–Ω–¥–æ–≤ –≤ —Å–∏—Å—Ç–µ–º–∞—Ö CDN. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç—Ä–µ–Ω–¥—ã –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å:

1. –ù–ê–ü–†–ê–í–õ–ï–ù–ò–ï —Ç—Ä–µ–Ω–¥–∞ (increasing/decreasing/stable)
2. –ü–†–ï–î–°–ö–ê–ó–ê–ù–ù–´–ï –ø—Ä–æ–±–ª–µ–º—ã
3. –ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
4. –£–í–ï–†–ï–ù–ù–û–°–¢–¨ –≤ –ø—Ä–æ–≥–Ω–æ–∑–µ –æ—Ç 0 –¥–æ 100

–û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –±—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º. –í–ê–ñ–ù–û: –û—Ç–≤–µ—á–∞–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{
  "trend_direction": "increasing|decreasing|stable",
  "predicted_issues": ["–ø—Ä–æ–±–ª–µ–º–∞ 1", "–ø—Ä–æ–±–ª–µ–º–∞ 2"],
  "optimization_suggestions": ["–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ 1", "–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ 2"],
  "confidence_score": 85
}"""},
                    {"role": "user", "content": context}
                ],
                max_tokens=1500,
                temperature=0.7
            )
            
            # –ü–∞—Ä—Å–∏–º JSON –æ—Ç–≤–µ—Ç
            import json
            content = response.choices[0].message.content
            data = json.loads(content)
            
            return AITrendAnalysis(
                trend_direction=data.get("trend_direction", "stable"),
                predicted_issues=data.get("predicted_issues", []),
                optimization_suggestions=data.get("optimization_suggestions", []),
                confidence_score=data.get("confidence_score", 0)
            )
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤: {e}")
            return AITrendAnalysis(
                trend_direction="stable",
                predicted_issues=[f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}"],
                optimization_suggestions=[],
                confidence_score=0
            )
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    async def ask_question(self, question: str) -> str:
        """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –æ—Ç–≤–µ—Ç–æ–º –Ω–∞ –≤–æ–ø—Ä–æ—Å"""
        try:
            context = await self._prepare_context_data()
            full_context = f"–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {question}\n\n–î–∞–Ω–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã:\n{context}"

            response = await self.client.chat.completions.create(
                model=self.config.openai.model,
                messages=[
                    {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ DevOps –∏ —Å–∏—Å—Ç–µ–º–∞–º CDN. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –±—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∏ –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–º."},
                    {"role": "user", "content": full_context}
                ],
                max_tokens=1500,
                temperature=0.7
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ AI –∞–Ω–∞–ª–∏–∑–∞ –≤–æ–ø—Ä–æ—Å–∞: {e}")
            return f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}"
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    async def analyze_code(self, configs_info: str) -> str:
        """–ê–Ω–∞–ª–∏–∑ –∫–æ–¥–∞ –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π"""
        try:
            response = await self.client.chat.completions.create(
                model=self.config.openai.model,
                messages=[
                    {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –∫–æ–¥–∞ –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π —Å–∏—Å—Ç–µ–º CDN. –ù–∞–π–¥–∏ –æ—à–∏–±–∫–∏, –ø—Ä–æ–±–ª–µ–º—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –Ω–µ–æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏ –¥–∞–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."},
                    {"role": "user", "content": f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:\n\n{configs_info}"}
                ],
                max_tokens=1500,
                temperature=0.7
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–¥–∞: {e}")
            return f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–¥–∞: {e}"
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    async def find_issues(self, debug_info: str) -> str:
        """–ü–æ–∏—Å–∫ –ø—Ä–æ–±–ª–µ–º –≤ —Å–∏—Å—Ç–µ–º–µ"""
        try:
            response = await self.client.chat.completions.create(
                model=self.config.openai.model,
                messages=[
                    {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–µ —Å–∏—Å—Ç–µ–º CDN. –ò—â–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã, –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è, –ø—Ä–æ–±–ª–µ–º—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –æ—à–∏–±–∫–∏ –≤ –ª–æ–≥–∞—Ö. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ."},
                    {"role": "user", "content": f"–ù–∞–π–¥–∏ –ø—Ä–æ–±–ª–µ–º—ã –≤ —Å–∏—Å—Ç–µ–º–µ:\n\n{debug_info}"}
                ],
                max_tokens=1500,
                temperature=0.7
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø—Ä–æ–±–ª–µ–º: {e}")
            return f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø—Ä–æ–±–ª–µ–º: {e}"
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    async def get_suggestions(self, current_state: str) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        try:
            response = await self.client.chat.completions.create(
                model=self.config.openai.model,
                messages=[
                    {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–∏—Å—Ç–µ–º CDN. –ü—Ä–µ–¥–æ—Å—Ç–∞–≤—å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏, –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –∞–ª–µ—Ä—Ç—ã, –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –±—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∏ –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–º."},
                    {"role": "user", "content": f"–î–∞–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:\n\n{current_state}"}
                ],
                max_tokens=1500,
                temperature=0.7
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {e}")
            return f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {e}"
    
    async def get_recommendations(self) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        try:
            # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—É—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
            metrics = await self.prometheus.get_all_metrics()
            containers = await self.docker.get_containers_status()
            
            context = "=== –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò ===\n\n"
            context += "–¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã:\n"
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã
            running_containers = [c for c in containers if c['status'] == 'running']
            stopped_containers = [c for c in containers if c['status'] != 'running']
            
            context += f"–†–∞–±–æ—Ç–∞—é—â–∏—Ö –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤: {len(running_containers)}\n"
            context += f"–û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤: {len(stopped_containers)}\n"
            
            if stopped_containers:
                context += f"–û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ: {', '.join([c['name'] for c in stopped_containers])}\n"
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏
            if metrics.get('redis'):
                redis_metrics = metrics['redis']
                context += f"\nRedis –º–µ—Ç—Ä–∏–∫–∏: {redis_metrics}\n"
            
            if metrics.get('nginx'):
                nginx_metrics = metrics['nginx']
                context += f"Nginx –º–µ—Ç—Ä–∏–∫–∏: {nginx_metrics}\n"
            
            response = await self.client.chat.completions.create(
                model=self.config.openai.model,
                messages=[
                    {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–∏—Å—Ç–µ–º CDN. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏, –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –∞–ª–µ—Ä—Ç—ã, –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –±—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∏ –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–º."},
                    {"role": "user", "content": context}
                ],
                max_tokens=self.config.openai.max_tokens,
                temperature=self.config.openai.temperature
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {e}")
            return f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {e}"
    
    async def _prepare_context_data(self) -> str:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è AI –∞–Ω–∞–ª–∏–∑–∞"""
        try:
            # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ —Å–∏—Å—Ç–µ–º–µ
            metrics = await self.prometheus.get_all_metrics()
            containers = await self.docker.get_containers_status()
            health_checks = await self.docker.get_health_checks()
            cache_stats = await self.docker.get_cache_statistics()
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è GPT
            context = "=== –ê–ù–ê–õ–ò–ó –°–ò–°–¢–ï–ú–´ CDN ===\n\n"
            
            # –°—Ç–∞—Ç—É—Å –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤
            context += "üê≥ –ö–û–ù–¢–ï–ô–ù–ï–†–´:\n"
            for container in containers:
                status_emoji = "‚úÖ" if container['status'] == 'running' else "‚ùå"
                context += f"{status_emoji} {container['name']}: {container['status']}\n"
                if container.get('health'):
                    context += f"   Health: {container['health']}\n"
            context += "\n"
            
            # Health checks
            context += "üè• HEALTH CHECKS:\n"
            for service, status in health_checks.items():
                status_emoji = "‚úÖ" if status['healthy'] else "‚ùå"
                context += f"{status_emoji} {service}: {status['status']}\n"
            context += "\n"
            
            # –ú–µ—Ç—Ä–∏–∫–∏ Nginx
            if metrics.get('nginx'):
                nginx_metrics = metrics['nginx']
                context += "üåê NGINX –ú–ï–¢–†–ò–ö–ò:\n"
                for metric, value in nginx_metrics.items():
                    if value and value != 'N/A':
                        context += f"   {metric}: {value}\n"
                context += "\n"
            
            # –ú–µ—Ç—Ä–∏–∫–∏ Redis
            if metrics.get('redis'):
                redis_metrics = metrics['redis']
                context += "üíæ REDIS –ú–ï–¢–†–ò–ö–ò:\n"
                for metric, value in redis_metrics.items():
                    if value and value != 'N/A':
                        context += f"   {metric}: {value}\n"
                context += "\n"
            
            # –ú–µ—Ç—Ä–∏–∫–∏ WebP
            if metrics.get('webp'):
                webp_metrics = metrics['webp']
                context += "üñºÔ∏è WEBP –ö–û–ù–í–ï–†–¢–ï–†:\n"
                for metric, value in webp_metrics.items():
                    if value and value != 'N/A':
                        context += f"   {metric}: {value}\n"
                context += "\n"
            
            # –°–∏—Å—Ç–µ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            if metrics.get('system'):
                system_metrics = metrics['system']
                context += "üíª –°–ò–°–¢–ï–ú–ù–´–ï –ú–ï–¢–†–ò–ö–ò:\n"
                for metric, value in system_metrics.items():
                    if value and value != 'N/A':
                        context += f"   {metric}: {value}\n"
                context += "\n"
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–µ—à–∞
            if cache_stats:
                context += "üíæ –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ö–ï–®–ê:\n"
                if cache_stats.get('redis'):
                    redis_stats = cache_stats['redis']
                    context += "   Redis:\n"
                    for key, value in redis_stats.items():
                        context += f"     {key}: {value}\n"
                if cache_stats.get('nginx'):
                    nginx_stats = cache_stats['nginx']
                    context += "   Nginx:\n"
                    for key, value in nginx_stats.items():
                        context += f"     {key}: {value}\n"
                context += "\n"
            
            return context
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {e}")
            return f"–û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö: {e}"